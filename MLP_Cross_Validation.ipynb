{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es7ciHwyjdse"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(csv_path):\n",
        "    \"\"\"\n",
        "    Load the dataset made of labels and TMD features stored ar csv_path\n",
        "        - csv_path: String giving the path where the dataset is stored\n",
        "    Returns features and labels as Pandas DataFrames\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    labels = df.iloc[:, 0].values\n",
        "    features = df.iloc[:, 1:].values\n",
        "    return features, labels\n",
        "\n",
        "def standardize_features(features):\n",
        "  \"\"\"\n",
        "  Make every feature centered and scaled to unit variance\n",
        "      - features: Pandas DataFrame\n",
        "  \"\"\"\n",
        "  scaler = StandardScaler()\n",
        "  return scaler.fit_transform(features)"
      ],
      "metadata": {
        "id": "-rZIyZjZj2Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp(input_size, hidden_sizes, output_size=2):\n",
        "  \"\"\"\n",
        "  Defines a function to create a MLP model with variable architecture\n",
        "      -input_size, output_size : Integers giving the size of input and input\n",
        "      -hidden_size : List of integers corresponding to the hidden layers widths\n",
        "  \"\"\"\n",
        "  layers = []\n",
        "  sizes = [input_size] + hidden_sizes + [output_size]\n",
        "  for i in range(len(sizes) - 1):\n",
        "      layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "      if i < len(sizes) - 2:\n",
        "          layers.append(nn.ReLU())\n",
        "  return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "GYKwVNuckWwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation(model, features, labels, num_epochs=10, batch_size=32, learning_rate=0.001, num_splits=5):\n",
        "  \"\"\"\n",
        "  For a given model and dataset (features, labels) this funciton performs K_fold cross validation\n",
        "      -model: Pytorch MLP model with appropriate input and output sizes\n",
        "      -features, labels = Dataset in the form of two Pandas dataframes\n",
        "      -num_epoch, batch_size, learning_rate : usual NN parameters\n",
        "      -num_split : number of splits for the K_fold CV\n",
        "  Returns a list with the num_splits values of validation loss\n",
        "  \"\"\"\n",
        "  skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  all_val_losses = []\n",
        "\n",
        "  for fold, (train_index, test_index) in enumerate(skf.split(features, labels)):\n",
        "    train_features, test_features = features[train_index], features[test_index]\n",
        "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
        "\n",
        "    train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32),\n",
        "                                  torch.tensor(train_labels, dtype=torch.int32))\n",
        "    test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32),\n",
        "                                  torch.tensor(test_labels, dtype=torch.int32))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(batch_features)\n",
        "          loss = criterion(outputs, batch_labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for batch_features, batch_labels in test_loader:\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        val_losses.append(val_loss / len(test_loader))\n",
        "        print(f'Fold {fold + 1}, Epoch [{epoch + 1}/{num_epochs}], Test Loss: {val_losses[-1]}, Test Accuracy: {accuracy}')\n",
        "\n",
        "    all_val_losses.append(val_losses)\n",
        "\n",
        "  return all_val_losses\n"
      ],
      "metadata": {
        "id": "wBYh0ND8kK9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparameter_search(features, labels, num_layers_options, hidden_size_options, num_splits=5, num_epochs=10, batch_size=32, learning_rate=0.001):\n",
        "  \"\"\"\n",
        "  Performs hyperparameters search in terms of width and depth for the MLP\n",
        "      -features, labels, num_splits=5, num_epochs=10, batch_size=32, learning_rate=0.001 : same as cross_validation\n",
        "      -num_layers_options : list of integers corresponding to the number of hidden layers in each architecture to be tested\n",
        "      -hidden_size_options : list of tuples of integers giving the sizes of each hidden leayer in each architecture to be tested. Must be consistent with num_layers_options\n",
        "  \"\"\"\n",
        "\n",
        "  #Consistency check\n",
        "  assert len(num_layers_options) == len(hidden_size_options)\n",
        "  for hidden_size_option, num_layers_option in zip(hidden_size_options, num_layers_options):\n",
        "    assert len(hidden_size_option) == num_layers_option\n",
        "\n",
        "  best_loss = np.inf\n",
        "  best_model = None\n",
        "  best_hidden_size = None\n",
        "  best_num_layers = None\n",
        "\n",
        "  losses = {}\n",
        "\n",
        "  for hidden_size_option, num_layers_option in zip(hidden_size_options, num_layers_options):\n",
        "\n",
        "    assert len(hidden_size_option)==num_layers_option\n",
        "\n",
        "    model = create_mlp(features.shape[1], hidden_size_option * num_layers_option, len(np.unique(labels)))\n",
        "    print(f\"\\nHidden Size: {hidden_size_option}, Num Layers: {num_layers_option}\")\n",
        "\n",
        "    validation_loss = np.average(cross_validation(model, features, labels, num_epochs=num_epochs, batch_size=batch_size,\n",
        "                            learning_rate=learning_rate, num_splits=num_splits))\n",
        "\n",
        "    losses[(num_layers_option,hidden_size_option)] = validation_loss\n",
        "\n",
        "    if validation_loss < best_loss:\n",
        "        best_loss = validation_loss\n",
        "        best_model = model\n",
        "        best_hidden_size = hidden_size_option\n",
        "        best_num_layers = num_layers_option\n",
        "\n",
        "  print(\"\\nBest Model:\")\n",
        "  print(f\"Hidden Size: {best_hidden_size}, Num Layers: {best_num_layers}\")\n",
        "  return best_model\n"
      ],
      "metadata": {
        "id": "xUf0yUOnkvyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = 'path.csv'  # Replace with actual dataset path\n",
        "\n",
        "features, labels = load_dataset(csv_path)\n",
        "features = standardize_features(features)\n",
        "\n",
        "input_size = features.shape[1]\n",
        "\n",
        "# Define hyperparameter search space\n",
        "## Careful with the consistence between the two lists!! (Otherwise it will raise AssertionError)\n",
        "num_layers_options = [1, 1, 2, 2, 3, 3]\n",
        "hidden_size_options = [(64,), (128,), (64, 32), (128, 64), (128,64,32), (64,32,16)]\n",
        "\n",
        "\n",
        "best_model = hyperparameter_search(features, labels, num_layers_options, hidden_size_options)\n"
      ],
      "metadata": {
        "id": "TblnJdt8vCnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, features, labels):\n",
        "  \"\"\"\n",
        "  Evaluated the model on the whole dataset once the best model found.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      inputs = torch.tensor(features, dtype=torch.float32)\n",
        "      targets = torch.tensor(labels, dtype=torch.long)\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      accuracy = (predicted == targets).sum().item() / len(targets)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "rCNUNqE2kzP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the entire dataset\n",
        "final_accuracy = evaluate_model(best_model, features, labels)\n",
        "print(f'\\nFinal Accuracy on the Entire Dataset: {final_accuracy}')"
      ],
      "metadata": {
        "id": "b63PCsBhlF5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}